{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "### Homework for Statistical Methods in Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100, 4) (100, 10) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# generate the 2 classes data including the 4-dimentional data and 10-dimentional data\n",
    "import numpy as np\n",
    "\n",
    "# generate data without noise\n",
    "mu, sigma = 0, 1\n",
    "train_1_pure = np.random.normal(mu, sigma, (100, 4))\n",
    "train_0_pure = np.zeros((100, 4))\n",
    "count = 0\n",
    "while count < 100:\n",
    "    temp_data = np.random.normal(mu, sigma, (1, 4))\n",
    "    if temp_data.dot(np.transpose(temp_data)) > 9 and \\\n",
    "        temp_data.dot(np.transpose(temp_data)) < 16:\n",
    "        train_0_pure[count, :] = temp_data\n",
    "        count += 1\n",
    "\n",
    "# generate data with Gaussian noise ~ N(0,1)\n",
    "mu, sigma = 0, 1\n",
    "train_1_noise = np.random.normal(mu, sigma, (100, 10))\n",
    "train_0_noise = np.zeros((100, 10))\n",
    "count = 0\n",
    "while count < 100:\n",
    "    temp_data = np.random.normal(mu, sigma, (1, 10))\n",
    "    if np.sum(temp_data[:4]**2) < 16 and np.sum(temp_data[:4]**2) > 9:\n",
    "        train_0_noise[count, :] = temp_data\n",
    "        count += 1\n",
    "\n",
    "print train_1_pure.shape, train_0_pure.shape, train_1_noise.shape, train_0_noise.shape\n",
    "\n",
    "# generate 1000 test data without noise \n",
    "# which includes 500 data labeled 1 and 500 data labeled 0\n",
    "test_1_pure = np.random.normal(mu, sigma, (500, 4))\n",
    "test_0_pure = np.zeros((500, 4))\n",
    "count = 0\n",
    "while count < 500:\n",
    "    temp_data = np.random.normal(mu, sigma, (1, 4))\n",
    "    if temp_data.dot(np.transpose(temp_data)) > 9 and \\\n",
    "        temp_data.dot(np.transpose(temp_data)) < 16:\n",
    "        test_0_pure[count, :] = temp_data\n",
    "        count += 1\n",
    "test_data_pure = np.vstack((test_1_pure, test_0_pure))\n",
    "test_label_pure = np.ones(1000)\n",
    "test_label_pure[500:] *= 0\n",
    "\n",
    "# generate 1000 test data with noise \n",
    "# which includes 500 data labeled 1 and 500 data labeled 0\n",
    "test_1_noise = test_1_pure = np.random.normal(mu, sigma, (500, 10))\n",
    "test_0_noise = np.zeros((500, 10))\n",
    "count = 0\n",
    "while count < 500:\n",
    "    temp_data = np.random.normal(mu, sigma, (1, 10))\n",
    "    if np.sum(temp_data[:4]**2) < 16 and np.sum(temp_data[:4]**2) > 9:\n",
    "        test_0_noise[count, :] = temp_data\n",
    "        count += 1\n",
    "test_data_noise = np.vstack((test_1_noise, test_0_noise))\n",
    "test_label_noise = np.ones(1000)\n",
    "test_label_noise[500:] *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 536.91083443906507, 'gamma': 0.003891124657703934, 'class_weight': 'balanced'}\n",
      "\n",
      "Best estimator's score:\n",
      "\n",
      "0.975\n",
      "\n",
      "Result for Test data without noise\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.98      0.93       500\n",
      "        1.0       0.98      0.88      0.93       500\n",
      "\n",
      "avg / total       0.94      0.93      0.93      1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy\n",
    "\n",
    "train_data_pure = np.vstack((train_1_pure, train_0_pure))\n",
    "train_label_pure = np.ones(200)\n",
    "train_label_pure[100:] *= 0\n",
    "\n",
    "svm_pure = svm.SVC(kernel='rbf')\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
    "  'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "# run randomized search with cross-validation\n",
    "n_iter_search = 1000\n",
    "random_search = RandomizedSearchCV(svm_pure, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=5)\n",
    "random_search.fit(train_data_pure, train_label_pure)\n",
    "\n",
    "print \"Best parameters set found on development set:\"\n",
    "print \"\"\n",
    "print random_search.best_params_\n",
    "print \"\"\n",
    "print \"Best estimator's score:\"\n",
    "print \"\"\n",
    "print random_search.best_score_\n",
    "print \"\"\n",
    "\n",
    "# test the result of parameter searching \n",
    "print \"Result for Test data without noise\"\n",
    "pure_pred = random_search.predict(test_data_pure)\n",
    "print classification_report(test_label_pure, pure_pred)\n",
    "print \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SVM model:\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision in Train dataset\n",
      "0.875\n",
      "Precision in Test dataset\n",
      "0.589\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'kernel': 'rbf', 'C': 4.6823712812494005, 'gamma': 0.7181712920337191}\n",
      "\n",
      "Best estimator's score:\n",
      "\n",
      "0.625\n",
      "\n",
      "Result for Test data with noise\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.77      0.68       500\n",
      "        1.0       0.69      0.49      0.57       500\n",
      "\n",
      "avg / total       0.64      0.63      0.63      1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_noise = np.vstack((train_1_noise, train_0_noise))\n",
    "train_label_noise = np.ones(200)\n",
    "train_label_noise[100:] *= 0\n",
    "\n",
    "svm_noise = svm.SVC(kernel='rbf')\n",
    "# using default parameter to fit the SVM model\n",
    "print \"Default SVM model:\"\n",
    "print \"\"\n",
    "default_model = svm_noise.fit(train_data_noise, train_label_noise)\n",
    "print default_model\n",
    "print \"Precision in Train dataset\"\n",
    "print default_model.score(train_data_noise, train_label_noise)\n",
    "print \"Precision in Test dataset\"\n",
    "print default_model.score(test_data_noise, test_label_noise)\n",
    "print \"\\n\"\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'C': scipy.stats.uniform(loc=0.5,scale=5), \n",
    "              'gamma': scipy.stats.uniform(loc=0.01, scale=5),\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "# run randomized search with cross-validation\n",
    "n_iter_search = 1000\n",
    "random_search_noise = RandomizedSearchCV(svm_noise, param_distributions=param_dist,\n",
    "                                         n_iter=n_iter_search, cv=5)\n",
    "random_search_noise.fit(train_data_noise, train_label_noise)\n",
    "\n",
    "print \"Best parameters set found on development set:\"\n",
    "print \"\"\n",
    "print random_search_noise.best_params_\n",
    "print \"\"\n",
    "print \"Best estimator's score:\"\n",
    "print \"\"\n",
    "print random_search_noise.best_score_\n",
    "print \"\"\n",
    "\n",
    "# test the result of parameter searching \n",
    "print \"Result for Test data with noise\"\n",
    "noise_pred = random_search_noise.predict(test_data_noise)\n",
    "print classification_report(test_label_noise, noise_pred)\n",
    "print \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification for Part 2\n",
    "From the result showing in the above, we find that even if using RBF kernel, the SVM classifier meets the problem of overfitting or the curse of dimention. \n",
    "\n",
    "Once adding the extra useless features to the data, we could see the much worse results of the test data given by the SVM classifier using RBF kernel. Howervr, what makes me at a loss is that by searching the parameter the best estimator's score is only 0.625 in dataset of training. What's more, the default model of the SVC's score is 0.875 in data set of training. After reflecting on the problem, I found the default model's score in dataset of testing is so low. Finally, I think it's because we use the whole dataset to train the default model but cross-validation methods to find the best parameter.  \n",
    "\n",
    "Back to the curse of dimention, the svm classifier using RBF kernel has advantage of controling the feature number use in the model by modifying the parameter gamma. However, we couldn't prohibit it from overfitting or meet the curse of dimention, once too much noise has been added into the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using poly kernel:\n",
      "\n",
      "Test score of data without noise:\n",
      "0.915\n",
      "\n",
      "Test score of data with noise:\n",
      "0.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM using poly kernel\n",
    "\n",
    "poly_pure = svm.SVC(kernel='poly', degree=8).fit(train_data_pure, train_label_pure)\n",
    "poly_noise = svm.SVC(kernel='poly', degree=6).fit(train_data_noise, train_label_noise)\n",
    "\n",
    "print 'SVM using poly kernel:'\n",
    "print ''\n",
    "print \"Test score of data without noise:\" \n",
    "print poly_pure.score(test_data_pure, test_label_pure)\n",
    "print ''\n",
    "print 'Test score of data with noise:'\n",
    "print poly_noise.score(test_data_noise, test_label_noise)\n",
    "print ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification for Part 3\n",
    "Using the default parameter, we find the SVM classifier which uses liner kernel doesn't surpass the SVM model using the RBF kernel. Also the liner kernel will meet the problem of overfitting or curse of dimention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
